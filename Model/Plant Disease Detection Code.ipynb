{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, models  # datsets  , transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Install tqdm for progress bars if not already installed\n",
    "# !pip install tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"GPU not available. Training will use CPU (slower).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "TESTING_MODE = True  # Set to False for full training\n",
    "SUBSET_PERCENTAGE = 0.10  # Use 10% of dataset when testing\n",
    "NUM_EPOCHS = 2 if TESTING_MODE else 5\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4  # Parallel data loading (0 if you have issues on Windows)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Testing Mode: {TESTING_MODE}\")\n",
    "if TESTING_MODE:\n",
    "    print(f\"  Using {SUBSET_PERCENTAGE*100}% of dataset\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Dataset Link (Plant Vliiage Dataset ):</b><br> <a href='https://data.mendeley.com/datasets/tywbtsjrjv/1'> https://data.mendeley.com/datasets/tywbtsjrjv/1 </a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\"Dataset\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply subset for testing mode\n",
    "if TESTING_MODE:\n",
    "    subset_size = int(len(dataset) * SUBSET_PERCENTAGE)\n",
    "    dataset = torch.utils.data.Subset(dataset, range(subset_size))\n",
    "    print(f\"Testing mode: Using {len(dataset)} images ({SUBSET_PERCENTAGE*100}% of full dataset)\")\n",
    "else:\n",
    "    print(f\"Full training mode: Using all {len(dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(np.floor(0.85 * len(dataset)))  # train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = int(np.floor(0.70 * split))  # validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(0, validation, split, len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"length of train size :{validation}\")\n",
    "print(f\"length of validation size :{split - validation}\")\n",
    "print(f\"length of test size :{len(dataset)-validation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, validation_indices, test_indices = (\n",
    "    indices[:validation],\n",
    "    indices[validation:split],\n",
    "    indices[split:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "validation_sampler = SubsetRandomSampler(validation_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_size = len(dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Convolution Aithmetic Equation : </b>(W - F + 2P) / S + 1 <br>\n",
    "W = Input Size<br>\n",
    "F = Filter Size<br>\n",
    "P = Padding Size<br>\n",
    "S = Stride <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for params in model.parameters():\n",
    "#     params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = model.classifier[0].in_features\n",
    "# n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.classifier = nn.Sequential(\n",
    "#     nn.Linear(n_features, 1024),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(0.4),\n",
    "#     nn.Linear(1024, targets_size),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, K):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # conv1\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            # conv2\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            # conv3\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "            # conv4\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(50176, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, K),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.conv_layers(X)\n",
    "\n",
    "        # Flatten\n",
    "        out = out.view(-1, 50176)\n",
    "\n",
    "        # Fully connected\n",
    "        out = self.dense_layers(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(targets_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # this include softmax + cross entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(model, criterion, train_loader, test_laoder, epochs):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    validation_losses = np.zeros(epochs)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        \n",
    "        # Add progress bar for training\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {e+1}/{epochs} [Train]\")\n",
    "        for inputs, targets in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs)\n",
    "\n",
    "            loss = criterion(output, targets)\n",
    "\n",
    "            train_loss.append(loss.item())  # torch to numpy world\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.3f}'})\n",
    "\n",
    "        train_loss = np.mean(train_loss)\n",
    "\n",
    "        validation_loss = []\n",
    "\n",
    "        # Add progress bar for validation\n",
    "        pbar_val = tqdm(validation_loader, desc=f\"Epoch {e+1}/{epochs} [Val]\")\n",
    "        for inputs, targets in pbar_val:\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            output = model(inputs)\n",
    "\n",
    "            loss = criterion(output, targets)\n",
    "\n",
    "            validation_loss.append(loss.item())  # torch to numpy world\n",
    "            \n",
    "            pbar_val.set_postfix({'loss': f'{loss.item():.3f}'})\n",
    "\n",
    "        validation_loss = np.mean(validation_loss)\n",
    "\n",
    "        train_losses[e] = train_loss\n",
    "        validation_losses[e] = validation_loss\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "\n",
    "        print(\n",
    "            f\"Epoch : {e+1}/{epochs} Train_loss:{train_loss:.3f} Val_loss:{validation_loss:.3f} Duration:{dt}\"\n",
    "        )\n",
    "\n",
    "    return train_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with optimization settings\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, sampler=train_sampler,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, sampler=test_sampler,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, sampler=validation_sampler,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(validation_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nStarting training for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "train_losses, validation_losses = batch_gd(\n",
    "    model, criterion, train_loader, validation_loader, NUM_EPOCHS\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_filename = 'plant_disease_model_1.pt'\n",
    "torch.save(model.state_dict(), model_filename)\n",
    "print(f\"✓ Model saved to: {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_size = 39\n",
    "model = CNN(targets_size)\n",
    "model.load_state_dict(torch.load(\"plant_disease_model_1_latest.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses , label = 'train_loss')\n",
    "plt.plot(validation_losses , label = 'validation_loss')\n",
    "plt.xlabel('No of Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(loader):\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "        n_correct += (predictions == targets).sum().item()\n",
    "        n_total += targets.shape[0]\n",
    "\n",
    "    acc = n_correct / n_total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = accuracy(train_loader)\n",
    "test_acc = accuracy(test_loader)\n",
    "validation_acc = accuracy(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Train Accuracy : {train_acc}\\nTest Accuracy : {test_acc}\\nValidation Accuracy : {validation_acc}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Testing Mode: {TESTING_MODE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total samples: {len(dataset)}\")\n",
    "print(f\"  Number of classes: {targets_size}\")\n",
    "print(f\"  Train samples: {validation}\")\n",
    "print(f\"  Validation samples: {split - validation}\")\n",
    "print(f\"  Test samples: {len(dataset) - split}\")\n",
    "print(f\"\\nAccuracy:\")\n",
    "print(f\"  Train Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"  Validation Accuracy: {validation_acc:.4f} ({validation_acc*100:.2f}%)\")\n",
    "print(f\"  Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"\\nModel saved to: plant_disease_model_1.pt\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "if TESTING_MODE:\n",
    "    print(\"  1. Set TESTING_MODE = False in the configuration cell\")\n",
    "    print(\"  2. Re-run all cells for full training\")\n",
    "    print(\"  3. Full training will take 4-7 hours on CPU or 15-40 min on GPU\")\n",
    "else:\n",
    "    print(\"  ✓ Full training complete!\")\n",
    "    print(\"  1. Use the saved model for predictions\")\n",
    "    print(\"  2. Consider transfer learning with VGG16 for better accuracy\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_prediction(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((224, 224))\n",
    "    input_data = TF.to_tensor(image)\n",
    "    input_data = input_data.view((-1, 3, 224, 224))\n",
    "    output = model(input_data)\n",
    "    output = output.detach().numpy()\n",
    "    index = np.argmax(output)\n",
    "    print(\"Original : \", image_path[12:-4])\n",
    "    pred_csv = data[\"disease_name\"][index]\n",
    "    print(pred_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PlantDisease",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
